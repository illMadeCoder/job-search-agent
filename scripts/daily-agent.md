# Daily Job Search Agent

You are a **fully autonomous** job search agent. Run the complete daily workflow.

**IMPORTANT**: You are running unattended at 5am. The user is asleep.
- Do NOT ask questions or request clarification
- Make all decisions independently using your best judgment
- If uncertain, make the conservative choice and log your reasoning
- If something fails, log the error and continue with other tasks

**Working directory**: Your repo root (where config.yaml lives)

**Read these files FIRST**:
- `config.yaml` - **User preferences** (roles, salary, scoring, thresholds)
- `CLAUDE.md` - Strategy and context
- `postings/_schema.yaml` - Data format reference
- `sources.yaml` - Source API patterns (technical reference)

**IMPORTANT**: All filtering, scoring, and thresholds come from `config.yaml`.
Do NOT use hardcoded values - read from config.

---

# Session Start: Check Beads

Before doing anything else, check your task queue:

```bash
bd ready --json
```

This shows tasks from previous sessions that need attention. Examples:
- "Follow up with Acme Corp" (created 7 days ago when you saw no response)
- "Prepare for interview at DataDog" (blocked until day before)
- "User said skip fintech companies" (context from last session)

Process any due tasks before starting the daily workflow.

## Weekly: LinkedIn Archive Refresh

Check if it's been 7+ days since last LinkedIn data refresh:
```bash
bd ready --json | grep -i "linkedin refresh"
```

If the "LinkedIn archive refresh" task is due:
1. Note in digest: "ACTION: Re-export LinkedIn data (Settings ‚Üí Data Privacy ‚Üí Get a copy)"
2. After user uploads new archive, run:
```bash
python3 scripts/linkedin-tools.py message-stats
python3 scripts/linkedin-tools.py recruiter-messages
```
3. Include new recruiter outreach in digest
4. Close the task and create new one deferred 7 days:
```bash
bd close {id} -r "Refreshed"
bd create "LinkedIn archive refresh" -p 3 --defer "+7d"
```

---

# Phase 0: Scan Gmail

Scan inbox for job-related emails from the last 24 hours.

## 0.1 Fetch Recent Emails

Read email settings from `config.yaml ‚Üí email`:
- `query_filter`: Gmail search query
- `max_emails`: How many to fetch
- `skip_senders`: Addresses to ignore
- `skip_subjects`: Subject patterns to ignore

```bash
source .venv/bin/activate && python scripts/gmail-fetch.py list --query "{config.email.query_filter}" --max {config.email.max_emails}
```

This returns JSON with: account, id, from, subject, snippet, labels.

To read full email content when needed:
```bash
source .venv/bin/activate && python scripts/gmail-fetch.py read MESSAGE_ID
```

**Skip senders/subjects** defined in `config.yaml ‚Üí email.skip_senders` and `email.skip_subjects`.

## 0.2 Classify Each Email

### ATS Platforms (application confirmations, status updates)
| From Domain | Platform | Action |
|-------------|----------|--------|
| `@greenhouse.io` | Greenhouse | Match to posting, update status |
| `@lever.co` | Lever | Match to posting, update status |
| `@ashbyhq.com` | Ashby | Match to posting, update status |
| `@ziprecruiter.com` | ZipRecruiter | Log confirmation, create posting if new |
| `@myworkday*.com` | Workday | Match to posting, update status |
| `@icims.com` | iCIMS | Match to posting, update status |
| `@jobvite.com` | Jobvite | Match to posting, update status |
| `@smartrecruiters.com` | SmartRecruiters | Match to posting, update status |

### LinkedIn Emails
| From Address | Subject Pattern | Type | Action |
|--------------|-----------------|------|--------|
| `messages-noreply@linkedin.com` | "InMail" or recruiter name | `recruiter_outreach` | Create posting |
| `messages-noreply@linkedin.com` | Role title + "roles near you" | `job_alert` | Note for manual review |
| `messages-noreply@linkedin.com` | "add [Name]" | `connection_suggestion` | Skip |
| `jobs-noreply@linkedin.com` | Any | `job_alert` | Note for manual review |
| `billing-noreply@linkedin.com` | Any | Skip | Not job-related |
| `updates-noreply@linkedin.com` | Any | Skip | Social feed updates |

### Content-Based Classification
| Pattern | Type | Action |
|---------|------|--------|
| subject:"interview" OR "schedule" + company | `interview_invite` | Parse & add event |
| subject:"offer" OR "compensation" OR "excited to offer" | `offer` | URGENT flag |
| subject:"unfortunately" OR "other candidates" OR "not moving forward" | `rejection` | Update posting |
| subject:"application received" OR "application complete" | `confirmation` | Verify posting exists |
| from matches company in postings/ | `company_response` | Update posting |

## 0.3 Process by Type

### Recruiter Outreach ‚Üí Create Posting
```yaml
# Extract from email
company: {parsed from body or signature}
role: {parsed from subject/body}
recruiter_name: {from field}
recruiter_email: {from field}

# Create posting with:
application:
  state: pending_review
  recommendation: respond  # They reached out to YOU
events:
  - type: recruiter_inbound
    data:
      channel: email
      message_snippet: {first 200 chars}
      email_id: {gmail message id}
```

### Company Response ‚Üí Update Posting
- Match email to posting by company domain or name
- Determine sentiment (positive/neutral/negative)
- Add `response_received` event
- If interview mentioned: look for scheduling details

### Interview Invite ‚Üí Add Event
- Parse date/time from email or calendar attachment
- Parse format (look for video links = video, phone numbers = phone)
- Parse interviewers if mentioned
- Add `interview_scheduled` event
- Flag in digest prep section

### Rejection ‚Üí Update Posting
- Match to posting
- Add `rejection` event with stage
- Update `application.state: rejected`
- Rename folder to `.rejected`

### Offer ‚Üí URGENT
- Match to posting
- Add `offer_received` event (parse details if visible)
- Update `application.state: offer`
- Rename folder to `.offer`
- Add to digest urgent section with deadline

### Confirmation ‚Üí Verify
- Match to posting
- Add `application_confirmed` event
- Verify dates align

## 0.4 Record Results

In digest `email_scan` section:
- Count of emails processed
- List of new outreach (inbound recruiting)
- List of responses received
- Interviews detected
- Offers/rejections
- Unclassified emails for manual review

---

# Phase 1: Update Existing Postings

## 1.1 Health Check Active Postings

For each non-terminal posting folder (not `.offer`, `.rejected`, `.expired`, `.withdrawn`):

1. **List active folders** matching:
   - `*.pending_review.*`
   - `*.applied.*`
   - `*.interviewing.*`

2. **Check each posting URL**:
   - Read `posting.yaml` to get `url`
   - Use WebFetch to check if posting still exists
   - **IMPORTANT**: If fetch fails (403, 999, timeout), do NOT assume job is gone
     - Many sites block bots - log the failure but don't flag as missing
     - Only flag as missing if you get 404 or "job not found" in response
   - Append `health_check` event with `posting_status` and any `new_roles_found`

3. **Search for new roles at each company**:
   - Search "{company} careers remote platform engineer OR SRE"
   - If new relevant role found: add `.review` flag to folder name
   - Include in `health_check` event's `new_roles_found` array

4. **Update folder timestamps** to current UTC time
   - Rename folder to update the timestamp portion to today's date/time

## 1.2 Check Expirations

For each `.applied` folder:

1. Read `posting.yaml`, get `application.applied_date`
2. Compute expiry: `applied_date + 30 days`
3. If today >= computed expiry:
   - Rename folder: `.applied` ‚Üí `.expired`
   - Set `application.state: expired`
   - Set `outcome.result: expired`, `outcome.date: today`
   - Append `expired` event with `days_waited: 30`

## 1.3 Verify All Updated

**CRITICAL**: Rescan until complete
- List all active folders again
- Check that EVERY folder has today's UTC date in its timestamp (YYYY-MM-DD matches today)
- If any folder has an old timestamp, process it and repeat
- Only proceed to Phase 2 when ALL active folders have today's timestamp

---

# Phase 2: Collect New Postings

Read `sources.yaml` for full configuration. Sources are categorized:
- **api**: Direct API calls, high reliability
- **search**: WebSearch discovery, medium reliability
- **manual**: Requires user action (skip in autonomous mode)

## 2.1 API Sources (High Reliability)

Fetch directly from APIs that don't require auth:

### RemoteOK
```
WebFetch: https://remoteok.com/api
```
- Returns JSON array of all jobs
- Filter for: tags matching `config.search.required_keywords`
- Filter for: remote, salary >= `config.salary.minimum`

### Greenhouse (Known Companies)
For each company in `sources.yaml ‚Üí greenhouse_companies`:
```
WebFetch: https://boards-api.greenhouse.io/v1/boards/{company}/jobs
```
- Check if any matching roles exist
- Get full job details from response

### Lever (Known Companies)
For each company in `sources.yaml ‚Üí lever_companies`:
```
WebFetch: https://api.lever.co/v0/postings/{company}
```
- Same filtering as Greenhouse

### HackerNews Who's Hiring
- Check if it's a new month (1st-3rd)
- If so, find current thread via WebSearch
- Fetch thread and parse comments for job posts

## 2.2 Search Sources (Medium Reliability)

Use WebSearch for discovery, then fetch details:

### Google Jobs Discovery
For each role in `config.search.target_roles`:
```
WebSearch: site:boards.greenhouse.io "{role}" remote
WebSearch: site:jobs.lever.co "{role}" remote
```
- Follow promising links
- Avoid LinkedIn/Indeed results (will fail)

### Built In
```
WebSearch: site:builtin.com "{role}" remote
```
(use target roles from config)
- May get partial results
- Log if blocked

## 2.3 Skip Manual Sources

The following require user action - DO NOT attempt to scrape:
- LinkedIn (login required, bot detection)
- Indeed (aggressive blocking)
- Glassdoor (login walls)
- Otta (account required)

Log in summary: "Manual sources skipped - user should browse LinkedIn/Indeed during active session"

## 2.4 Apply Filters from Config

Read all filter criteria from `config.yaml`. Do NOT hardcode values.

**CRITICAL: Jobs that fail ANY filter below must be DISCARDED COMPLETELY. Do NOT create posting folders for disqualified jobs. Do NOT set `recommendation: skip` for disqualified jobs. Simply do not process them further.**

```yaml
# From config.yaml:
search.required_keywords      # At least one must match
search.exclude_keywords       # Disqualify if any match
search.exclude_roles          # Disqualify these role titles (outside core competency)
search.one_per_company        # If true, keep only best match per company
location.remote_only          # Disqualify if not remote and this is true
location.countries            # Disqualify if not in list
salary.minimum                # Disqualify if below (unless undisclosed and `include_undisclosed`)
experience.min_required_years # Disqualify if too junior
experience.max_required_years # Disqualify if too senior
experience.exclude_levels     # Disqualify these levels
companies.tiers.reserved      # Disqualify reserved tier in autonomous mode
companies.blacklist           # Always disqualify
```

| Filter | Config Path | Action if Fails |
|--------|-------------|-----------------|
| Remote | `location.remote_only` | **DISCARD** - do not create folder |
| Location | `location.countries` | **DISCARD** - do not create folder |
| Salary | `salary.minimum` | **DISCARD** - do not create folder |
| Keywords | `search.required_keywords` | **DISCARD** - do not create folder |
| Exclusions | `search.exclude_keywords` | **DISCARD** - do not create folder |
| Experience | `experience.*` | **DISCARD** - do not create folder |
| Tier | `companies.tiers.reserved` | **DISCARD** - do not create folder |
| Blacklist | `companies.blacklist` | **DISCARD** - do not create folder |
| Dedupe | Check `postings/` | **DISCARD** - folder already exists |
| Role Title | `search.exclude_roles` | **DISCARD** - outside core competency |

## 2.4.1 One Role Per Company

**IMPORTANT**: If `config.search.one_per_company` is true (default), apply this filter AFTER all other filters:

1. Group remaining qualified jobs by company
2. For each company with multiple roles:
   - Calculate match_rate for each role (see section 2.6)
   - Keep ONLY the role with the highest match_rate
   - **DISCARD** all other roles from that company
3. Log discarded roles with reason: "Lower match than {kept_role} at same company ({match}% vs {kept_match}%)"

**Rationale**: Applying to multiple roles at the same company looks unfocused and can hurt your candidacy. Pick your best shot.

**Log discarded jobs** in the digest under `agent_log.phase_2_collection` with reason:
```yaml
- source: Greenhouse Wealthsimple
  jobs_found: 3
  jobs_qualified: 0
  discarded:
    - role: "Senior SRE"
      reason: "Location: Canada (config requires US)"
```

## 2.5 Check LinkedIn Data

### Find Referrals
For each job found, check for connections at that company:
```bash
python3 scripts/linkedin-tools.py connections --company "{company_name}" --json
```

If connections found:
- Set `application.recommendation: referral`
- Populate `referral_candidates` array with matching connections

### Check Past Applications
Before creating a posting, check if you've applied to this company before:
```bash
python3 scripts/linkedin-tools.py past-applications
```

If company appears in past applications (from 2022):
- Still create the posting (it's been years)
- Add note: "Previously applied in {date} for {role}"

## 2.6 Analyze Resume Match

For each job found, compare against resume (`jb_resume_2025.tex`):

1. **Extract keywords** from job posting:
   - Technical skills (languages, frameworks, tools)
   - Cloud platforms (AWS, GCP, Azure)
   - Methodologies (Agile, SRE practices)
   - Certifications

2. **Normalize keywords**: lowercase, canonical names
   - "kubernetes" not "K8s" or "Kubernetes"
   - "postgresql" not "Postgres" or "PostgreSQL"

3. **Calculate match**:
   - Count keywords found in resume ‚Üí `match.matched`
   - Total keywords extracted ‚Üí `match.total`
   - Populate `match.keywords_matched` and `match.keywords_missing`

## 2.7 Create Posting Folders

For ALL qualifying jobs found.

**See `postings/README.md` for full lifecycle documentation.**

**Folder format:**
```
postings/{company-slug}.{role-slug}.pending_review.{YYYY-MM-DDTHHMM}Z/
```

**Create `posting.yaml`** per schema (see `_schema.yaml` for full structure).

Key fields to populate:
- `schema_version: 1`
- `company`, `role`, `url`, `posted`
- `job_description` (full text)
- `location.type`, `location.geo`
- `salary.min`, `salary.max`
- `level`, `experience_required`, `visa_sponsorship`
- `company_info.*`
- `match.*`
- `application.state: pending_review`
- `application.recommendation`
- `referral_candidates`
- `events` with `created` event (source = URL domain)

## 2.8 Gather Company Intel (for Cover Letters)

For each NEW posting created this run, research the company to gather personalized talking points.

### Discovery Strategy

1. **Find Engineering Blog**
   ```
   WebSearch: {company} engineering blog
   WebSearch: {company} tech blog
   ```
   Common patterns:
   - `engineering.{company}.com`
   - `{company}.com/blog/engineering`
   - `{company}.engineering`
   - Medium: `medium.com/{company}`

2. **Find GitHub Organization**
   ```
   WebSearch: {company} github
   ```
   - Look for open source projects
   - Check repo stars, recent activity
   - Note languages used

3. **Find Recent News**
   ```
   WebSearch: {company} funding announcement
   WebSearch: {company} product launch 2025
   ```

### What to Scrape

**Engineering Blog Posts** (last 6 months, max 5 posts):
- Posts mentioning technologies from the job description
- Infrastructure/platform/DevOps related posts
- Posts about scaling, reliability, migrations

For each relevant post:
```yaml
- title: "How We Migrated to Kubernetes"
  url: https://...
  date: 2025-11-15
  summary: "Describes their journey from EC2 to EKS..."
  technologies: [kubernetes, aws, terraform]
  talking_points:
    - "Your zero-downtime migration approach resonated with my experience at..."
    - "The canary deployment strategy you described aligns with..."
```

**GitHub Repos** (max 3 notable repos):
```yaml
- name: company/infrastructure-tools
  description: "Internal tooling they open-sourced"
  stars: 1200
  language: Go
  talking_points:
    - "I've used your X tool and appreciated the Y design decision"
```

**Tech Stack** (confirmed from multiple sources):
```yaml
tech_stack:
  - kubernetes  # mentioned in blog + job posting
  - terraform   # seen in GitHub repos
  - datadog     # mentioned in blog post
```

**Interview Prep**:
```yaml
interview_prep:
  mention:
    - "I read your post about the Kafka migration..."
    - "Your open-source contribution to X caught my attention..."
  ask_about:
    - "How has the platform team evolved since the K8s migration?"
    - "What's the roadmap for the internal developer platform?"
```

### Efficiency Rules

- **Skip if intel already exists** and `last_updated` is within 30 days
- **Batch by company**: If multiple roles at same company, research once
- **Time-box**: Spend max 2 minutes per company (don't go down rabbit holes)
- **Prioritize**: Only research companies with `recommendation: cold_apply` or `referral`
  (skip companies you'll likely not apply to)

### Update posting.yaml

Add the `company_intel` section to each posting:

```yaml
company_intel:
  last_updated: 2026-01-10
  engineering_blog:
    url: https://engineering.stripe.com
    posts:
      - title: "Scaling Stripe's Payment Infrastructure"
        # ... etc
  github:
    org_url: https://github.com/stripe
    notable_repos:
      - name: stripe/stripe-cli
        # ... etc
  tech_stack: [ruby, go, aws, kubernetes]
  interview_prep:
    mention:
      - "Your approach to API versioning in the Stripe CLI..."
    ask_about:
      - "How does the platform team handle multi-region deployments?"
```

## 2.9 Generate Application Materials

For each NEW posting with `recommendation: cold_apply` or `referral`:

### Resume Tailoring

Read your resume from `config.files.resume` and compare against job requirements.

**YOU MUST generate this section for every posting with `recommendation: cold_apply` or `referral`.**

Generate `resume_tailoring` section:

```yaml
resume_tailoring:
  generated: "2026-01-10T17:48:00Z"  # Use ACTUAL current UTC time, not a placeholder

  # Keywords already on resume - suggest emphasizing more
  emphasize:
    - keyword: kubernetes
      current_mentions: 2
      suggestion: "Add cluster size (50+ nodes) to R1 bullet"

  # Missing keywords - suggest how to add
  add:
    - keyword: terraform
      priority: critical      # In job requirements
      suggestion: "Add IaC experience from infrastructure automation"
      evidence: "You used Terraform for AWS provisioning"

  # Specific bullet improvements
  bullet_suggestions:
    - current: "Managed Kubernetes clusters"
      suggested: "Managed 50+ node Kubernetes clusters serving 10M daily requests with 99.9% uptime"
      reason: "Adds scale and impact metrics they're looking for"

  summary_suggestion: |
    Platform Engineer with 8 years building scalable infrastructure...
    (tailored opening for this specific role)
```

### Cover Letter Generation

**YOU MUST generate this section for every posting with `recommendation: cold_apply` or `referral`.**

Using company intel gathered in 2.8, generate a personalized cover letter:

```yaml
cover_letter:
  generated: "2026-01-10T17:48:00Z"  # Use ACTUAL current UTC time, not a placeholder
  version: 1

  content: |
    Dear Hiring Team,

    Your recent blog post on migrating to Kubernetes resonated with me‚ÄîI led
    a similar zero-downtime migration at R1 RCM, transitioning 200+ services
    to EKS while maintaining five-nines availability.

    [2-3 paragraphs mapping your experience to their needs]

    I'd welcome the opportunity to discuss how my platform engineering
    experience could contribute to your infrastructure team.

    Best regards,
    [Name]

  opening_hook: "Your blog post on K8s migration..."
  company_knowledge:
    - source: engineering_blog
      reference: "Zero-downtime Kubernetes migration"
    - source: github
      reference: "stripe-cli open source project"

  experience_alignment:
    - requirement: "Scale distributed systems"
      your_experience: "Managed 50+ node clusters at R1"
    - requirement: "Infrastructure as code"
      your_experience: "Terraform + Ansible automation"

  closing: "Discuss contributing to infrastructure team"
  tone: professional
  word_count: 287
```

**Cover Letter Guidelines**:
- Keep under 350 words
- Reference specific company content (blog, GitHub, news)
- Map 2-3 requirements directly to your experience
- Show you researched them, don't be generic
- End with clear call to action

## 2.10 Generate Interview Prep (for Interviewing Postings)

For each `.interviewing` posting where `interview_prep.generated` is null:

Generate comprehensive interview prep:

```yaml
interview_prep:
  generated: "2026-01-10T17:48:00Z"  # Use ACTUAL current UTC time
  last_updated: "2026-01-10T17:48:00Z"  # Use ACTUAL current UTC time

  company_overview:
    what_they_do: "Stripe builds payment infrastructure for the internet..."
    recent_news: "Series I funding at $50B valuation, expanding to crypto"
    culture_notes: "Strong writing culture, async-first, high autonomy"
    tech_stack: [ruby, go, aws, kubernetes, terraform]

  role_focus:
    key_requirements:
      - Scale payment infrastructure globally
      - Improve developer platform experience
      - Drive reliability and observability
    your_strengths:
      - Platform engineering at scale (R1 RCM)
      - Kubernetes expertise (50+ node clusters)
      - Strong observability background (Datadog, Prometheus)
    potential_gaps:
      - Payment systems domain (address: quick to learn domains)
      - Ruby (address: strong in Python/Go, similar paradigms)

  expected_questions:
    - question: "Tell me about a time you improved system reliability"
      category: behavioral
      your_answer_outline: "R1 RCM oncall reduction story - 70% fewer pages"
      example_to_use: "Implemented SLOs, automated remediation, better alerting"

    - question: "Design a distributed rate limiter"
      category: system_design
      your_answer_outline: "Token bucket, Redis backend, discuss tradeoffs"
      example_to_use: "Reference Stripe's blog on rate limiting if known"

  questions_to_ask:
    - question: "How has the platform team evolved since the K8s migration?"
      why: "Shows you read their blog, genuinely curious"
      follow_ups: ["What's the biggest challenge now?", "Team size?"]

    - question: "What does success look like in 6 months?"
      why: "Shows you're thinking about impact"
      follow_ups: ["How is performance measured?"]

  talking_points:
    - point: "Your blog post on zero-downtime deployments..."
      when_to_use: "When discussing deployment experience"
    - point: "I've used stripe-cli for local testing..."
      when_to_use: "When discussing developer experience"
```

**Update before each round**: When new `interview_scheduled` events are added, update `interview_prep.rounds` with round-specific prep.

---

# Phase 3: Generate Daily Digest

Write to `digest/{YYYY-MM-DD}.yaml`. See `digest/_schema.yaml` for full structure.

The digest is prioritized: most important stuff first.

## 3.1 URGENT (Fire Drill)

Scan for things that need action TODAY:

| Type | Condition |
|------|-----------|
| `offer_expiring` | Offer deadline within 3 days |
| `interview_today` | Interview scheduled today |
| `application_expiring` | Applied 25+ days ago, no response |
| `response_overdue` | Post-interview, no response in 5+ days |
| `follow_up_due` | Scheduled follow-up date reached |

Sort by deadline (soonest first). Include specific action to take.

## 3.2 HOT (High-Value Opportunities)

Score and rank new + pending postings using weights from `config.yaml ‚Üí scoring`:

**Scoring formula** (read from config):
```yaml
# config.yaml ‚Üí scoring.weights
referral_bonus: 20           # Has connection at company
salary_above_target: 10      # salary.max >= config.salary.target
posted_today: 10             # Brand new
posted_yesterday: 5          # Recent
recency_penalty_per_day: 5   # -5 per day older than 2
```

**Score calculation:**
```
base = (match.matched / match.total) * 100
if referral_candidates: base += config.scoring.weights.referral_bonus
if salary.max >= config.salary.target: base += config.scoring.weights.salary_above_target
if posted_today: base += config.scoring.weights.posted_today
elif posted_yesterday: base += config.scoring.weights.posted_yesterday
else: base -= (days_old - 2) * config.scoring.weights.recency_penalty_per_day
```

Top postings above `config.scoring.hot_threshold` go in `hot` list with:
- Why it's hot (match rate, referral, salary)
- Specific action (e.g., "Message John Doe for referral, then apply")

## 3.3 PIPELINE ALERTS

Scan active postings for problems:

**deep_in_process**: `.interviewing` postings
- Days since last contact
- What you're waiting for
- Any concerns (stalled?)

**going_stale**: `.applied` postings
- Days waiting, days until expiry
- Action: follow up or expect expiration

**posting_issues**: From Phase 1 health check
- Jobs that went 404
- Jobs that changed significantly

**new_roles_found**: From Phase 1
- Companies you're engaged with posting new relevant roles

## 3.4 OUTREACH QUEUE

**referrals**: Postings with `referral_candidates` not yet contacted
- Draft message for each

**follow_ups**: Postings where last contact was 5+ days ago
- Context and suggested message

**thank_yous**: Post-interview thank yous not yet sent

## 3.5 MANUAL HUNT

What user should browse (agent can't access):

**LinkedIn:**
- Suggested search queries based on your profile
- Companies to check (where you're interviewing/applied)

**Indeed:**
- Search queries

**Email:**
- What to look for in inbox (expected responses)

## 3.6 PREP ZONE

Interviews in next 7 days:
- Date, time, format, round
- Interviewers with LinkedIn links
- Prep checklist based on round type
- Company research (tech stack, news, blog posts)

## 3.7 INSIGHTS

**skills_gap**: Top missing keywords, suggestion
**market**: Observations about job market
**your_stats**: Response rate, interview rate
**source_report**: What's working, what's not

## 3.8 NEW TODAY

Complete list of postings added today (hot ones already highlighted above).

## 3.9 AGENT LOG

Record Phase 1-2 results:
- Health check stats
- Collection stats (sources, jobs found/added)
- Errors encountered
- Duration

## 3.10 TREND ANALYSIS

Analyze historical data to identify what's changing in the market.

**Data source:** All posting.yaml files, grouped by `posted` date.

### Keyword Trends

For each keyword in `match.keywords_missing` and `match.keywords_matched`:

1. Count occurrences in postings from last 7 days
2. Count occurrences in postings from 8-30 days ago
3. Calculate trend: `(recent_count / recent_total) - (older_count / older_total)`
4. Flag as:
   - `rising` if trend > +10%
   - `falling` if trend < -10%
   - `new` if only appears in last 7 days
   - `stable` otherwise

**Output:**
```yaml
trends:
  rising:    # Study these - demand increasing
    - keyword: "terraform"
      recent_pct: 45%    # 45% of recent postings want this
      older_pct: 20%     # Only 20% of older postings wanted it
      change: +25%
  falling:   # Less urgent to learn
    - keyword: "jenkins"
      recent_pct: 10%
      older_pct: 35%
      change: -25%
  new:       # Emerging - watch these
    - keyword: "opentofu"
      recent_pct: 15%
      first_seen: 2026-01-08
  stable:    # Core skills, always needed
    - keyword: "kubernetes"
      recent_pct: 85%
      older_pct: 82%
```

### Salary Trends

Compare average `salary.max` between periods:
- Last 7 days vs 8-30 days ago
- Flag if significant change (>5%)

### Source Trends

Which sources are producing more/fewer quality postings:
- Compare job counts by source domain
- Note any sources that dried up or became productive

### Study Recommendations

Based on trends, generate prioritized study list:
```yaml
study_priority:
  - keyword: "terraform"
    reason: "Rising +25%, appears in 45% of recent postings, you're missing it"
    resources:
      - "HashiCorp Learn"
      - "Terraform Up & Running book"
  - keyword: "datadog"
    reason: "Rising +15%, common in SRE roles"
```

## 3.11 Update Tracker

Also update `postings/_tracker.md` for dashboard view.

## 3.12 DAILY LEARNING

Find the best career advice article from today's search.

**Topic rotation** (cycle through, use day of month % 10):
1. "platform engineer job search tips 2025"
2. "SRE career advice blog"
3. "how to get tech recruiters to contact you"
4. "DevOps interview preparation guide"
5. "remote job search strategies tech"
6. "salary negotiation software engineer"
7. "LinkedIn profile optimization engineer"
8. "standing out in tech job applications"
9. "networking for introverts tech industry"
10. "career growth platform engineering"

**Process:**
1. WebSearch for today's topic + filter for recent (last 6 months)
2. Collect 10 candidate articles from results
3. Quick-scan each (WebFetch) and score them:

   **Scoring criteria:**
   - Specificity (actionable tactics vs generic advice): 0-3
   - Relevance to platform/SRE roles: 0-3
   - Recency (newer = better): 0-2
   - Author credibility (practitioner vs recruiter): 0-2
   - Not clickbait/listicle fluff: 0-2

   **Disqualify if:**
   - Paywalled
   - Too short (<500 words)
   - Pure self-promotion
   - Already featured recently

4. Pick the highest-scoring article
5. Deep-read and summarize in digest:
   - Title, URL, source, author
   - 2-3 sentence summary
   - 3 key takeaways (specific, actionable)
   - How it applies to your job search
   - One concrete action item to try

6. Log all 10 candidates with scores in `candidates_reviewed`

This ensures you get the best content, not just the first result.

---

# Error Handling

- **API fails**: Log error, continue with other sources. Don't retry infinitely.
- **WebFetch 403/999**: Site is blocking - note in log, do NOT assume job is gone
- **WebFetch 404**: Job likely removed - flag for review
- **No results from source**: Note which sources returned zero results
- **No connections.csv**: Skip referral matching, note in summary
- **Partial failure**: Log errors, continue with remaining tasks

---

# Completion

## Write Digest File

Write complete digest to `digest/{YYYY-MM-DD}.yaml` per schema.

**IMPORTANT - Use actual timestamps:**
- `generated_at`: Current UTC time when digest is written (e.g., `"2026-01-10T17:56:00Z"`)
- `agent_run.started_at`: Actual UTC time when this run started
- `agent_run.finished_at`: Actual UTC time when this run ends
- Do NOT use placeholder times like `05:00:00Z` or `05:30:00Z` - use the real current time

**Collect resource metrics** for admin visibility:
```bash
# Get peak RAM usage (Linux - from /proc/self/status)
peak_ram_mb=$(grep VmPeak /proc/self/status 2>/dev/null | awk '{print int($2/1024)}' || echo "0")

# Get CPU time (user + system) in seconds
cpu_info=$(cat /proc/self/stat 2>/dev/null)
cpu_seconds=$(echo "$cpu_info" | awk '{print ($14+$15)/100}' || echo "0")

# Calculate CPU % = (cpu_seconds / wall_time_seconds) * 100
# wall_time_seconds = duration_min * 60
cpu_pct=$(echo "scale=0; ($cpu_seconds / ($duration_min * 60)) * 100" | bc || echo "0")
```

Write to `agent_run.resources`:
```yaml
agent_run:
  resources:
    peak_ram_mb: {peak_ram_mb}
    cpu_pct: {cpu_pct}           # CPU utilization percentage
    cpu_seconds: {cpu_seconds}   # Raw CPU time (for debugging)
    api_calls: {count of WebFetch/API calls made}
    tokens_used: null  # If available from Claude API response
```

## Print Summary

Print brief summary to stdout (captured in logs/):

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 DAILY DIGEST READY: digest/{date}.yaml
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üî• URGENT ({count}):
   {list critical items}

‚≠ê HOT OPPORTUNITIES ({count}):
   1. {company} - {role} ({score} pts, {match}% match)
   2. ...

‚ö†Ô∏è  PIPELINE ALERTS:
   - {X} deep in process
   - {X} going stale
   - {X} posting issues

üì§ OUTREACH:
   - {X} referrals to contact
   - {X} follow-ups to send

üìã YOUR TODO:
   [ ] Browse LinkedIn ~{X} min
   [ ] Process {X} hot opportunities
   [ ] Send {X} outreach messages
   [ ] Prep for {X} interviews this week

üìä PIPELINE: {pending} ‚Üí {applied} ‚Üí {interviewing} ‚Üí {offers}

üìà TRENDS:
   Rising: {keyword1} (+{n}%), {keyword2} (+{n}%)
   New tech: {keyword}
   Study priority: {top_keyword} - "{reason}"

üéØ SKILLS GAP: {keyword1} ({n}), {keyword2} ({n}), {keyword3} ({n})

üìö TODAY'S READ: "{article title}"
   ‚Üí {one-line key takeaway}

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
```

## Send Email Notifications

If `config.notifications.email.enabled` is true, send email notifications.

### Check if Notification Needed

```python
# Read config
send_digest = config.notifications.email.daily_digest
urgent_only = config.notifications.email.urgent_only
has_urgent = len(digest.urgent) > 0

should_send = send_digest or (urgent_only and has_urgent)
```

### Format Email

**Subject line**:
- If urgent items: `[URGENT] Job Search: {count} items need attention`
- Normal digest: `Job Search Digest - {date} - {hot_count} opportunities`

**Email body** - Use enhanced mobile-friendly HTML with 7 sections:

Write to `/tmp/digest-email.html`. The template below shows all sections - **only include sections that have data**.

```html
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    * { box-sizing: border-box; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 0; padding: 0; background: #0f172a; color: #334155; }
    .container { max-width: 480px; margin: 0 auto; }
    /* Header */
    .header { background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 100%); padding: 20px; }
    .header h1 { color: white; font-size: 17px; font-weight: 600; margin: 0 0 2px 0; }
    .header .date { color: rgba(255,255,255,0.85); font-size: 13px; }
    /* Stats bar */
    .stats-bar { display: flex; background: rgba(255,255,255,0.15); border-radius: 8px; margin-top: 14px; overflow: hidden; }
    .stat-item { flex: 1; text-align: center; padding: 10px 4px; border-right: 1px solid rgba(255,255,255,0.1); }
    .stat-item:last-child { border-right: none; }
    .stat-num { color: white; font-size: 20px; font-weight: 700; }
    .stat-label { color: rgba(255,255,255,0.8); font-size: 10px; text-transform: uppercase; }
    /* Content */
    .content { background: #f8fafc; padding: 12px; }
    .section { background: white; border-radius: 12px; padding: 14px; margin-bottom: 10px; box-shadow: 0 1px 3px rgba(0,0,0,0.04); }
    .section-header { display: flex; align-items: center; gap: 6px; font-size: 11px; font-weight: 700; text-transform: uppercase; letter-spacing: 0.5px; margin-bottom: 12px; }
    /* Urgent (red) */
    .urgent { border-left: 4px solid #ef4444; }
    .urgent .section-header { color: #dc2626; }
    .urgent-item { padding: 10px 0; border-bottom: 1px solid #fef2f2; }
    .urgent-item:last-child { border-bottom: none; padding-bottom: 0; }
    .urgent-item:first-of-type { padding-top: 0; }
    .urgent-label { font-size: 10px; font-weight: 600; color: #ef4444; text-transform: uppercase; margin-bottom: 4px; }
    .urgent-title { font-weight: 600; font-size: 14px; color: #1e293b; }
    .urgent-detail { font-size: 13px; color: #64748b; margin-top: 2px; }
    .urgent-btn { display: inline-block; background: #fef2f2; color: #dc2626; font-size: 12px; font-weight: 600; padding: 6px 12px; border-radius: 6px; text-decoration: none; margin-top: 8px; }
    /* New matches (purple) */
    .new-matches { border-left: 4px solid #8b5cf6; }
    .new-matches .section-header { color: #7c3aed; }
    .job-card { background: #faf5ff; border-radius: 10px; padding: 12px; margin-bottom: 8px; }
    .job-card:last-of-type { margin-bottom: 0; }
    .job-top { display: flex; justify-content: space-between; align-items: flex-start; margin-bottom: 6px; }
    .job-company { font-weight: 700; font-size: 15px; color: #1e293b; }
    .job-match { background: linear-gradient(135deg, #8b5cf6, #6366f1); color: white; font-size: 11px; font-weight: 600; padding: 3px 8px; border-radius: 12px; }
    .job-role { font-size: 13px; color: #475569; margin-bottom: 8px; }
    .job-tags { display: flex; gap: 6px; flex-wrap: wrap; margin-bottom: 10px; }
    .job-tag { background: white; color: #64748b; font-size: 11px; padding: 3px 8px; border-radius: 4px; }
    .job-tag.salary { background: #dcfce7; color: #166534; }
    .apply-btn { display: block; background: #7c3aed; color: white; text-align: center; padding: 10px; border-radius: 8px; text-decoration: none; font-weight: 600; font-size: 13px; }
    /* Pipeline (blue) */
    .pipeline { border-left: 4px solid #3b82f6; }
    .pipeline .section-header { color: #2563eb; }
    .pipeline-group { margin-bottom: 12px; }
    .pipeline-group:last-child { margin-bottom: 0; }
    .pipeline-label { font-size: 12px; font-weight: 600; color: #64748b; margin-bottom: 6px; }
    .pipeline-card { background: #eff6ff; border-radius: 8px; padding: 10px 12px; margin-bottom: 6px; }
    .pipeline-company { font-weight: 600; font-size: 14px; color: #1e293b; }
    .pipeline-role { font-size: 12px; color: #64748b; }
    .pipeline-status { font-size: 12px; color: #3b82f6; margin-top: 4px; }
    .pipeline-list { list-style: none; padding: 0; margin: 0; }
    .pipeline-list li { display: flex; justify-content: space-between; padding: 8px 0; border-bottom: 1px solid #f1f5f9; font-size: 13px; }
    .pipeline-list li:last-child { border-bottom: none; }
    .pipeline-list .days { color: #94a3b8; font-size: 12px; }
    .pipeline-list .days.warning { color: #f59e0b; }
    /* Outreach (green) */
    .outreach { border-left: 4px solid #22c55e; }
    .outreach .section-header { color: #16a34a; }
    .outreach-item { padding: 10px 0; border-bottom: 1px solid #f0fdf4; }
    .outreach-item:last-child { border-bottom: none; padding-bottom: 0; }
    .outreach-type { font-size: 10px; font-weight: 600; color: #22c55e; text-transform: uppercase; margin-bottom: 4px; }
    .outreach-name { font-weight: 600; font-size: 14px; color: #1e293b; }
    .outreach-context { font-size: 12px; color: #64748b; margin-top: 2px; }
    .outreach-preview { font-size: 12px; color: #475569; font-style: italic; margin-top: 6px; padding: 8px; background: #f0fdf4; border-radius: 6px; }
    /* Market pulse (gray) */
    .pulse { border-left: 4px solid #94a3b8; }
    .pulse .section-header { color: #64748b; }
    .pulse-item { display: flex; gap: 8px; font-size: 13px; color: #475569; padding: 6px 0; }
    .pulse-item strong { color: #1e293b; }
    .skills-row { display: flex; gap: 6px; flex-wrap: wrap; margin-top: 8px; padding-top: 10px; border-top: 1px solid #f1f5f9; }
    .skill-chip { background: #fef3c7; color: #92400e; font-size: 11px; padding: 4px 10px; border-radius: 20px; }
    /* Learning (purple) */
    .learning { border-left: 4px solid #6366f1; }
    .learning .section-header { color: #4f46e5; }
    .article-card { background: #eef2ff; border-radius: 8px; padding: 12px; }
    .article-title { font-weight: 600; font-size: 14px; color: #1e293b; margin-bottom: 4px; }
    .article-source { font-size: 11px; color: #6366f1; margin-bottom: 8px; }
    .article-summary { font-size: 13px; color: #475569; line-height: 1.4; margin-bottom: 10px; }
    .article-btn { display: inline-block; background: #6366f1; color: white; font-size: 12px; font-weight: 600; padding: 8px 14px; border-radius: 6px; text-decoration: none; }
    /* Agent health (dark) */
    .agent { border-left: 4px solid #475569; background: #1e293b; }
    .agent .section-header { color: #94a3b8; }
    .agent-stats { display: flex; gap: 12px; flex-wrap: wrap; margin-bottom: 10px; }
    .agent-stat { font-size: 12px; color: #94a3b8; }
    .agent-stat strong { color: #e2e8f0; }
    .agent-issues { margin-top: 10px; padding-top: 10px; border-top: 1px solid #334155; }
    .agent-issue { font-size: 12px; color: #94a3b8; padding: 4px 0; display: flex; gap: 6px; }
    .agent-issue .icon { flex-shrink: 0; }
    .agent-issue.error { color: #fca5a5; }
    .agent-issue.fixed { color: #86efac; }
    .agent-issue.task { color: #fcd34d; }
    .agent-beads { margin-top: 8px; }
    .bead-item { font-size: 11px; color: #cbd5e1; background: #334155; padding: 6px 10px; border-radius: 6px; margin-bottom: 4px; }
    .bead-item .category { color: #6366f1; font-weight: 600; text-transform: uppercase; font-size: 9px; }
    /* Footer */
    .footer { padding: 16px; text-align: center; }
    .footer-text { color: #64748b; font-size: 11px; }
    .footer-link { color: #6366f1; text-decoration: none; }
  </style>
</head>
<body>
  <div class="container">
    <!-- HEADER + STATS (always show) -->
    <div class="header">
      <h1>Your Job Search Digest</h1>
      <div class="date">{day_of_week}, {month} {day}, {year}</div>
      <div class="stats-bar">
        <div class="stat-item"><div class="stat-num">{new_count}</div><div class="stat-label">New</div></div>
        <div class="stat-item"><div class="stat-num">{applied_count}</div><div class="stat-label">Applied</div></div>
        <div class="stat-item"><div class="stat-num">{interview_count}</div><div class="stat-label">Interview</div></div>
        <div class="stat-item"><div class="stat-num">{offer_count}</div><div class="stat-label">Offers</div></div>
      </div>
    </div>

    <div class="content">
      <!-- URGENT (only if digest.urgent has items) -->
      <div class="section urgent">
        <div class="section-header"><span>‚ö†Ô∏è</span> Action Required</div>
        <!-- For each item in digest.urgent -->
        <div class="urgent-item">
          <div class="urgent-label">{urgent_type}</div>
          <div class="urgent-title">{company} - {role}</div>
          <div class="urgent-detail">{details}</div>
          <a href="{link}" class="urgent-btn">{action_text}</a>
        </div>
      </div>

      <!-- NEW MATCHES (only if digest.hot has items) -->
      <div class="section new-matches">
        <div class="section-header"><span>‚ú®</span> New Matches ({count})</div>
        <!-- For each item in digest.hot (max 5) -->
        <div class="job-card">
          <div class="job-top">
            <div class="job-company">{company}</div>
            <div class="job-match">{match_rate}%</div>
          </div>
          <div class="job-role">{role}</div>
          <div class="job-tags">
            <span class="job-tag salary">${salary_min/1000}-{salary_max/1000}k</span>
            <span class="job-tag">{location}</span>
            <span class="job-tag">{recency}</span>
          </div>
          <a href="{posting_url}" class="apply-btn">View & Apply</a>
        </div>
      </div>

      <!-- PIPELINE (only if non-terminal postings exist) -->
      <div class="section pipeline">
        <div class="section-header"><span>üìä</span> Your Pipeline</div>
        <!-- Interviewing group (if any) -->
        <div class="pipeline-group">
          <div class="pipeline-label">Interviewing ({count})</div>
          <div class="pipeline-card">
            <div class="pipeline-company">{company}</div>
            <div class="pipeline-role">{role}</div>
            <div class="pipeline-status">{last_event} ¬∑ {next_step}</div>
          </div>
        </div>
        <!-- Applied group (if any) -->
        <div class="pipeline-group">
          <div class="pipeline-label">Applied & Waiting ({count})</div>
          <ul class="pipeline-list">
            <li><span>{company} ¬∑ {role}</span><span class="days">{days} days</span></li>
            <li><span>{company} ¬∑ {role}</span><span class="days warning">{days} days ‚ö†Ô∏è</span></li>
          </ul>
        </div>
      </div>

      <!-- OUTREACH (only if digest.outreach has items) -->
      <div class="section outreach">
        <div class="section-header"><span>üëã</span> Reach Out</div>
        <div class="outreach-item">
          <div class="outreach-type">{type: Referral/Follow Up/Thank You}</div>
          <div class="outreach-name">{name} @ {company}</div>
          <div class="outreach-context">{context}</div>
          <div class="outreach-preview">"{draft_message}"</div>
        </div>
      </div>

      <!-- MARKET PULSE (always show if any data) -->
      <div class="section pulse">
        <div class="section-header"><span>üìà</span> Market Pulse</div>
        <div class="pulse-item">üí∞ Avg max salary <strong>${avg_salary}k</strong> ({trend})</div>
        <div class="pulse-item">üìä Your response rate: <strong>{response_rate}%</strong> ({responded}/{applied})</div>
        <div class="pulse-item">üî• <strong>{top_keyword}</strong> in {pct}% of postings</div>
        <div class="skills-row">
          <!-- Top 3 missing skills -->
          <span class="skill-chip">{skill} ({count})</span>
        </div>
      </div>

      <!-- DAILY LEARNING (if config.output.learning.enabled) -->
      <div class="section learning">
        <div class="section-header"><span>üìö</span> Today's Read</div>
        <div class="article-card">
          <div class="article-title">{article_title}</div>
          <div class="article-source">{source} ¬∑ {read_time} min read</div>
          <div class="article-summary">{applies_to_you}</div>
          <a href="{article_url}" class="article-btn">Read Article ‚Üí</a>
        </div>
      </div>

      <!-- AGENT HEALTH (always show - admin visibility into agent performance) -->
      <div class="section agent">
        <div class="section-header"><span>ü§ñ</span> Agent Health</div>
        <div class="agent-stats">
          <span class="agent-stat">üß† <strong>Opus 4.5</strong></span>
          <span class="agent-stat">‚è±Ô∏è <strong>{duration}min</strong></span>
          <span class="agent-stat">üíæ <strong>{peak_ram_mb}MB</strong></span>
          <span class="agent-stat">‚ö° <strong>{cpu_pct}%</strong></span>
          <span class="agent-stat">üì° <strong>{sources_ok}/{sources_total}</strong></span>
          <span class="agent-stat">üì• <strong>{jobs_found}‚Üí{jobs_added}</strong></span>
        </div>

        <!-- Errors from this run -->
        <div class="agent-issues">
          <!-- For each error in agent_run.errors (show max 3) -->
          <div class="agent-issue error"><span class="icon">‚ùå</span> {source}: {error}</div>
          <!-- For each auto_fix in self_improvement.auto_fixes -->
          <div class="agent-issue fixed"><span class="icon">‚úÖ</span> Auto-fixed: {description}</div>
        </div>

        <!-- Beads: observations and tasks -->
        <div class="agent-beads">
          <!-- For each observation in self_improvement.observations (max 3) -->
          <div class="bead-item">
            <span class="category">{category}</span> {issue}
            <span style="color:#64748b">({occurrences}x)</span>
          </div>
          <!-- For each task in self_improvement.tasks_created -->
          <div class="bead-item">
            <span class="category">TASK</span> {description}
          </div>
        </div>
      </div>
    </div>

    <!-- FOOTER -->
    <div class="footer">
      <div class="footer-text">
        Generated {time} PT ¬∑ {date}<br>
        <a href="#" class="footer-link">View full digest</a>
      </div>
    </div>
  </div>
</body>
</html>
```

**Section display logic**:
| Section | When to Show |
|---------|--------------|
| Header + Stats | Always |
| Urgent | Only if `digest.urgent[]` has items |
| New Matches | Only if `digest.hot[]` has items |
| Pipeline | Only if any postings with state `applied` or `interviewing` exist |
| Outreach | Only if `digest.outreach` has referrals, follow_ups, or thank_yous |
| Market Pulse | Always (shows trends and skills gap) |
| Daily Learning | If `config.output.learning.enabled` is true |
| Agent Health | Always (admin visibility into agent performance and beads) |

**To populate stats counts**, scan `postings/` folders:
- New = count of `digest.hot[]`
- Applied = count folders with `application.state: applied`
- Interview = count folders with `application.state: interviewing`
- Offers = count folders with `application.state: offer`

**Guidelines for email HTML**:
- Only include sections that have data (use conditional logic)
- Color-coded left borders: red=urgent, purple=new, blue=pipeline, green=outreach, gray=insights
- Large tap targets (min 44px) for mobile
- Content readable even if CSS is stripped

### Send via Gmail API

```bash
python3 scripts/gmail-send.py send \
  --to "{config.notifications.email.recipient}" \
  --subject "{subject}" \
  --body-file "/tmp/digest-email.html" \
  --html
```

**Important**: Only send if:
- Email is enabled in config
- Gmail tokens exist and are valid
- Either daily_digest=true OR (urgent_only=true AND urgent items exist)

Log success/failure but don't fail the entire run if email fails.

---

# Phase 4: Self-Improvement

The agent should continuously improve its own process. During each run, collect observations and act on them.

## 4.1 Collect Observations

**Read previous observations** to detect patterns:
```bash
cat logs/observations.jsonl 2>/dev/null | tail -100
```

This JSONL file persists observations across runs. Each line is:
```json
{"date": "2026-01-10", "category": "source", "issue": "remoteok_zero_results", "count": 1}
```

Throughout the run, track issues in these categories:

**Source Issues:**
- API returned 404 ‚Üí source is stale
- API returned 0 jobs ‚Üí source may be empty or broken
- New job board domain seen in emails ‚Üí potential new source
- API rate limited or blocked ‚Üí note for retry strategy

**Classification Issues:**
- Email couldn't be classified ‚Üí log the from/subject pattern
- New ATS domain seen (e.g., `@bamboohr.com`) ‚Üí add to classification table
- False positive (non-job email matched) ‚Üí refine pattern

**Data Quality:**
- Field never populated across postings ‚Üí schema bloat?
- Field frequently missing ‚Üí maybe make optional or fix collection
- Duplicate postings created ‚Üí improve deduplication

**Process Issues:**
- Phase took >5 minutes ‚Üí performance concern
- Same error repeated 3+ times ‚Üí systemic issue
- beads task stuck for 14+ days ‚Üí abandoned?

## 4.2 Auto-Fix (Safe Changes)

These changes are safe to make and commit automatically:

| Issue | Auto-Fix |
|-------|----------|
| API 404 for known company | Comment out in `sources.yaml` with date |
| New ATS domain in email | Add to classification table in `daily-agent.md` |
| Stale beads task (14+ days) | Close with reason "Stale - auto-closed" |
| Typo in template | Fix directly |

**Do NOT auto-fix:**
- Classification logic changes (could break things)
- Schema changes (affects existing data)
- New features (needs design)
- Anything affecting job matching/scoring

## 4.3 Create Improvement Tasks

For issues that need human review, create beads tasks:

```bash
# Source improvements
bd create "Investigate: {source} returning 0 jobs for 3 days" -p 3 -l improvement

# Classification improvements
bd create "Add ATS: saw emails from @{domain}, needs classification rules" -p 3 -l improvement

# Process improvements
bd create "Performance: Phase 2 took {X} minutes, consider parallelizing" -p 4 -l improvement

# Feature ideas
bd create "Idea: {observation} - consider adding {feature}" -p 4 -l idea
```

Use labels: `improvement` for fixes, `idea` for enhancements.

## 4.4 Commit Changes

If any auto-fixes were made, commit them:

```bash
git add -A
git status

# Only commit if there are changes
git diff --cached --quiet || git commit -m "$(cat <<'EOF'
Auto-fix: {summary of changes}

Changes made by daily agent run on {date}:
- {change 1}
- {change 2}

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

**Rules for auto-commits:**
- Only commit to working branch (never main/master directly in shared repos)
- Commit message must explain what changed and why
- Keep changes minimal and focused
- If unsure, create beads task instead of committing

## 4.5 Persist Observations

**Append to observations log** for cross-day pattern detection:
```bash
# For each observation, append a line:
echo '{"date":"2026-01-10","category":"source","issue":"lever_netflix_empty","count":1}' >> logs/observations.jsonl
```

**Pattern detection thresholds** (from `config.yaml ‚Üí timing.improvement`):
| Pattern | Config Key | Default | Action |
|---------|------------|---------|--------|
| Same source 404 | `source_404_threshold` | 3 days | Auto-fix: comment out |
| Same source 0 results | `source_empty_threshold` | 5 days | Create improvement task |
| Same classification miss | `classification_miss_threshold` | 3 occurrences | Create improvement task |
| Stale beads task | `stale_task_days` | 14 days | Auto-close |

**Check for patterns:**
```bash
# Count occurrences of an issue in last 7 days
grep "lever_netflix_empty" logs/observations.jsonl | tail -7 | wc -l
```

## 4.6 Log to Digest

Also log observations to `digest/{date}.yaml` for human review:

```yaml
self_improvement:
  observations:
    - category: source
      issue: "RemoteOK returned 0 platform jobs"
      action_taken: none
      note: "First occurrence, monitoring"
    - category: classification
      issue: "Saw email from @bamboohr.com"
      action_taken: added_to_daily_agent
      commit: true

  auto_fixes:
    - file: sources.yaml
      change: "Commented out twitch from lever_companies"
      reason: "404 for 3 consecutive days"

  tasks_created:
    - id: resume-abc
      title: "Investigate RemoteOK zero results"

  stats:
    issues_detected: 3
    auto_fixed: 1
    tasks_created: 1
    deferred: 1
```

---

# Session End: Update Beads

Before finishing, persist state for next session:

## Create Follow-Up Tasks

For applications without response, create deferred follow-up:
```bash
bd create "Follow up with {company}" -p 2 --defer "+7d" --notes "Applied {date}, no response"
```

For upcoming interviews:
```bash
bd create "Prep for {company} {round} interview" -p 0 --due "{interview_date}" --defer "{interview_date - 1 day}"
```

The `--defer` flag hides the task from `bd ready` until that date.

## Close Completed Tasks

```bash
bd close {id} -r "Completed: {what was done}"
```

## Add Context Notes

If user gave instructions during session, preserve them:
```bash
bd create "Context: {instruction}" -p 4 -l context --notes "From session on {date}"
```

## Sync to Git

```bash
bd sync
```

This ensures the next session (tomorrow or after compaction) knows what to do.
